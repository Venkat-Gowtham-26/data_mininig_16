{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T03:59:01.101738Z",
     "iopub.status.busy": "2024-12-02T03:59:01.101172Z",
     "iopub.status.idle": "2024-12-02T04:00:30.339047Z",
     "shell.execute_reply": "2024-12-02T04:00:30.338134Z",
     "shell.execute_reply.started": "2024-12-02T03:59:01.101681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      "review_id      500 non-null object\n",
      "user_id        500 non-null object\n",
      "business_id    500 non-null object\n",
      "stars          500 non-null float64\n",
      "useful         500 non-null int64\n",
      "funny          500 non-null int64\n",
      "cool           500 non-null int64\n",
      "text           500 non-null object\n",
      "date           500 non-null object\n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 35.3+ KB\n",
      "None\n",
      "                review_id                 user_id             business_id  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0    3.0       0      0     0   \n",
      "1    5.0       1      0     1   \n",
      "2    3.0       0      0     0   \n",
      "3    5.0       1      0     1   \n",
      "4    4.0       1      0     1   \n",
      "\n",
      "                                                text                 date  \n",
      "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n",
      "1  I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n",
      "2  Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
      "4  Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# If using Kaggle API, you can use the dataset directly by setting up the kaggle.json file\n",
    "# Ensure you've uploaded the kaggle.json file containing API credentials into the environment.\n",
    "# Use Kaggle API to download the dataset if needed (as an alternative to direct upload)\n",
    "\n",
    "# If you already have the file loaded, use it:\n",
    "file_path = r'C:\\Users\\91939\\Downloads\\yelp_academic_dataset_review (1).json'  # Replace with your actual file path\n",
    "\n",
    "# Load data into DataFrame\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data and convert to pandas DataFrame.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load the dataset\n",
    "reviews_df = load_data(file_path)\n",
    "\n",
    "# Display basic info about dataset\n",
    "print(reviews_df.info())\n",
    "print(reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:00:30.340837Z",
     "iopub.status.busy": "2024-12-02T04:00:30.340487Z",
     "iopub.status.idle": "2024-12-02T04:00:37.329788Z",
     "shell.execute_reply": "2024-12-02T04:00:37.328807Z",
     "shell.execute_reply.started": "2024-12-02T04:00:30.340804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (156, 10)\n"
     ]
    }
   ],
   "source": [
    "# Label positive (4-5 stars) and negative (1-2 stars)\n",
    "def label_sentiment(stars):\n",
    "    if stars >= 4:\n",
    "        return 'positive'\n",
    "    elif stars <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'  # Optional: If you want to include neutral, otherwise filter it out\n",
    "\n",
    "# Apply sentiment labeling\n",
    "reviews_df['sentiment'] = reviews_df['stars'].apply(label_sentiment)\n",
    "\n",
    "# Filter out neutral reviews (optional based on your analysis requirement)\n",
    "reviews_df = reviews_df[reviews_df['sentiment'] != 'neutral']\n",
    "\n",
    "# Balance the classes (for faster training)\n",
    "positive_reviews = reviews_df[reviews_df['sentiment'] == 'positive']\n",
    "negative_reviews = reviews_df[reviews_df['sentiment'] == 'negative']\n",
    "\n",
    "# Dynamically determine the sample size\n",
    "min_samples = min(positive_reviews.shape[0], negative_reviews.shape[0])\n",
    "\n",
    "# Randomly sample to limit to 200,000 total samples (balanced)\n",
    "positive_sample = positive_reviews.sample(n=min_samples, random_state=42)\n",
    "negative_sample = negative_reviews.sample(n=min_samples, random_state=42)\n",
    "\n",
    "# Combine the balanced sample\n",
    "reviews_df = pd.concat([positive_sample, negative_sample])\n",
    "\n",
    "print(f\"Balanced dataset shape: {reviews_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:00:37.331081Z",
     "iopub.status.busy": "2024-12-02T04:00:37.330817Z",
     "iopub.status.idle": "2024-12-02T04:00:50.875695Z",
     "shell.execute_reply": "2024-12-02T04:00:50.874626Z",
     "shell.execute_reply.started": "2024-12-02T04:00:37.331053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature matrix: (156, 2992)\n",
      "Top 25 features: ['00' '000' '10' ... 'yum' 'yummy' 'zero']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorization with a maximum of 25 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(reviews_df['text'])\n",
    "\n",
    "print(\"Shape of the feature matrix:\", X.shape)\n",
    "print(\"Top 25 features:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:00:50.877484Z",
     "iopub.status.busy": "2024-12-02T04:00:50.876995Z",
     "iopub.status.idle": "2024-12-02T04:01:52.626333Z",
     "shell.execute_reply": "2024-12-02T04:01:52.625360Z",
     "shell.execute_reply.started": "2024-12-02T04:00:50.877445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced feature matrix shape: (156, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Reduce dimensionality to 50 components (adjust if needed for faster results)\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "\n",
    "# Fit and transform the TF-IDF data\n",
    "X_reduced = pca.fit_transform(X.toarray())\n",
    "# Save the PCA model\n",
    "joblib.dump(pca, 'pca_model.pkl')\n",
    "\n",
    "print(f\"Reduced feature matrix shape: {X_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:01:52.628895Z",
     "iopub.status.busy": "2024-12-02T04:01:52.628585Z",
     "iopub.status.idle": "2024-12-02T04:08:46.154128Z",
     "shell.execute_reply": "2024-12-02T04:08:46.152055Z",
     "shell.execute_reply.started": "2024-12-02T04:01:52.628866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.013712012782767777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91939\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\cluster\\_kmeans.py:1044: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  \"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Apply MiniBatchKMeans clustering\n",
    "kmeans = MiniBatchKMeans(n_clusters=5, max_iter=100, tol=0.01, batch_size=5000, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "# Evaluate clustering quality using silhouette score\n",
    "sil_score = silhouette_score(X_reduced, clusters)\n",
    "print(f\"Silhouette Score: {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:08:46.155745Z",
     "iopub.status.busy": "2024-12-02T04:08:46.155305Z",
     "iopub.status.idle": "2024-12-02T04:08:51.078011Z",
     "shell.execute_reply": "2024-12-02T04:08:51.077107Z",
     "shell.execute_reply.started": "2024-12-02T04:08:46.155688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce the dimensionality to 2 components for visualization\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "reduced_data = pca_2d.fit_transform(X_reduced)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title('MiniBatchKMeans Clustering of Yelp Reviews')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:08:51.079386Z",
     "iopub.status.busy": "2024-12-02T04:08:51.079094Z",
     "iopub.status.idle": "2024-12-02T04:08:51.770939Z",
     "shell.execute_reply": "2024-12-02T04:08:51.769983Z",
     "shell.execute_reply.started": "2024-12-02T04:08:51.079351Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the MiniBatchKMeans model and TF-IDF vectorizer\n",
    "joblib.dump(kmeans, 'minibatch_kmeans_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save the predictions\n",
    "predictions_df = pd.DataFrame({'review_id': reviews_df['review_id'], 'cluster': clusters})\n",
    "predictions_df.to_csv('clustering_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:14:19.900870Z",
     "iopub.status.busy": "2024-12-02T04:14:19.900481Z",
     "iopub.status.idle": "2024-12-02T04:14:19.950304Z",
     "shell.execute_reply": "2024-12-02T04:14:19.949180Z",
     "shell.execute_reply.started": "2024-12-02T04:14:19.900836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (124, 50)\n",
      "Test set shape: (32, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Assuming 'sentiment' is the target column and 'X_reduced' is the feature matrix\n",
    "y = reviews_df['sentiment']  # Target labels (positive/negative)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the training and testing data\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:14:23.871129Z",
     "iopub.status.busy": "2024-12-02T04:14:23.870173Z",
     "iopub.status.idle": "2024-12-02T04:14:25.764836Z",
     "shell.execute_reply": "2024-12-02T04:14:25.763059Z",
     "shell.execute_reply.started": "2024-12-02T04:14:23.871093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test data: ['negative' 'positive' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'negative' 'negative']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Output prediction results\n",
    "print(\"Predictions on test data:\", y_pred[:10])  # Display first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:14:25.770199Z",
     "iopub.status.busy": "2024-12-02T04:14:25.768598Z",
     "iopub.status.idle": "2024-12-02T04:14:27.373339Z",
     "shell.execute_reply": "2024-12-02T04:14:27.372397Z",
     "shell.execute_reply.started": "2024-12-02T04:14:25.770138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.76      0.79        17\n",
      "    positive       0.75      0.80      0.77        15\n",
      "\n",
      "    accuracy                           0.78        32\n",
      "   macro avg       0.78      0.78      0.78        32\n",
      "weighted avg       0.78      0.78      0.78        32\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  4]\n",
      " [ 3 12]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:14:27.374933Z",
     "iopub.status.busy": "2024-12-02T04:14:27.374544Z",
     "iopub.status.idle": "2024-12-02T04:14:28.293851Z",
     "shell.execute_reply": "2024-12-02T04:14:28.292727Z",
     "shell.execute_reply.started": "2024-12-02T04:14:27.374893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and data have been saved to /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Kaggle output folder path (Kaggle automatically stores output here)\n",
    "save_dir = '/kaggle/working'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Save the MiniBatchKMeans model\n",
    "joblib.dump(kmeans, os.path.join(save_dir, 'minibatch_kmeans_model.pkl'))\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, os.path.join(save_dir, 'tfidf_vectorizer.pkl'))\n",
    "\n",
    "# Save the Logistic Regression model\n",
    "joblib.dump(logreg, os.path.join(save_dir, 'logistic_regression_model.pkl'))\n",
    "\n",
    "# Save the feature matrix (X_reduced) and target labels (y)\n",
    "joblib.dump(X_reduced, os.path.join(save_dir, 'X_reduced.pkl'))\n",
    "joblib.dump(y, os.path.join(save_dir, 'y_labels.pkl'))\n",
    "\n",
    "# Save clustering predictions (for review)\n",
    "predictions_df = pd.DataFrame({'review_id': reviews_df['review_id'], 'cluster': clusters})\n",
    "predictions_df.to_csv(os.path.join(save_dir, 'clustering_predictions.csv'), index=False)\n",
    "\n",
    "print(f\"Models and data have been saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:14:28.295963Z",
     "iopub.status.busy": "2024-12-02T04:14:28.295652Z",
     "iopub.status.idle": "2024-12-02T04:14:28.557130Z",
     "shell.execute_reply": "2024-12-02T04:14:28.553736Z",
     "shell.execute_reply.started": "2024-12-02T04:14:28.295933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Logistic Regression Model: 255\n",
      "Error loading KMeans Model: 0\n",
      "Error loading TFIDF Vectorizer: No module named 'scipy.sparse._csr'\n",
      "Error loading PCA Model: 63\n",
      "Text: The food was amazing, the service was excellent, and I had a great time!\n",
      "Sentiment Prediction: positive\n",
      "Cluster Prediction: 3\n",
      "--------------------------------------------------\n",
      "Text: Worst experience ever, I will never come back to this restaurant.\n",
      "Sentiment Prediction: negative\n",
      "Cluster Prediction: 4\n",
      "--------------------------------------------------\n",
      "Text: It was an average experience. The food was okay but the service could be better.\n",
      "Sentiment Prediction: negative\n",
      "Cluster Prediction: 0\n",
      "--------------------------------------------------\n",
      "Text: I love this place, the ambiance is perfect, and the staff is friendly!\n",
      "Sentiment Prediction: positive\n",
      "Cluster Prediction: 4\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved models and vectorizers\n",
    "##logreg = joblib.load(r'C:\\Users\\91939\\Downloads\\logistic_regression_model.pkl')\n",
    "##kmeans = joblib.load(r'C:\\Users\\91939\\Downloads\\minibatch_kmeans_model.pkl')\n",
    "##tfidf_vectorizer = joblib.load(r'C:\\Users\\91939\\Downloads\\tfidf_vectorizer.pkl')\n",
    "##pca = joblib.load(r'C:\\Users\\91939\\Downloads\\pca_model.pkl')  # Load the saved PCA model\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "try:\n",
    "    logreg = load(r'C:\\Users\\91939\\Downloads\\logistic_regression_model.pkl')\n",
    "    print(\"Logistic Regression Model Loaded Successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Logistic Regression Model: {e}\")\n",
    "\n",
    "try:\n",
    "    kmeans = load(r'C:\\Users\\91939\\Downloads\\minibatch_kmeans_model.pkl')\n",
    "    print(\"KMeans Model Loaded Successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading KMeans Model: {e}\")\n",
    "\n",
    "try:\n",
    "    tfidf_vectorizer = load(r'C:\\Users\\91939\\Downloads\\tfidf_vectorizer.pkl')\n",
    "    print(\"TFIDF Vectorizer Loaded Successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading TFIDF Vectorizer: {e}\")\n",
    "\n",
    "try:\n",
    "    pca = load(r'C:\\Users\\91939\\Downloads\\pca_model.pkl')\n",
    "    print(\"PCA Model Loaded Successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PCA Model: {e}\")\n",
    "\n",
    "# Sentiment analysis function\n",
    "def sentiment_predictions(text):\n",
    "    text_transformed = tfidf_vectorizer.transform([text])\n",
    "    \n",
    "    # Apply PCA using the loaded PCA model\n",
    "    text_reduced = pca.transform(text_transformed.toarray())\n",
    "\n",
    "    sentiment = logreg.predict(text_reduced)[0]\n",
    "    return sentiment\n",
    "\n",
    "# Clustering function\n",
    "def cluster_predictions(text):\n",
    "    text_transformed = tfidf_vectorizer.transform([text])\n",
    "    \n",
    "    # Apply PCA using the loaded PCA model\n",
    "    text_reduced = pca.transform(text_transformed.toarray())\n",
    "\n",
    "    cluster = kmeans.predict(text_reduced)[0]\n",
    "    return cluster\n",
    "\n",
    "# Sample test function\n",
    "def run_sample_tests():\n",
    "    sample_texts = [\n",
    "        \"The food was amazing, the service was excellent, and I had a great time!\",\n",
    "        \"Worst experience ever, I will never come back to this restaurant.\",\n",
    "        \"It was an average experience. The food was okay but the service could be better.\",\n",
    "        \"I love this place, the ambiance is perfect, and the staff is friendly!\"\n",
    "    ]\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        print(f\"Text: {text}\")\n",
    "        \n",
    "        # Sentiment Prediction\n",
    "        sentiment = sentiment_predictions(text)\n",
    "        print(f\"Sentiment Prediction: {sentiment}\")\n",
    "        \n",
    "        # Cluster Prediction\n",
    "        cluster = cluster_predictions(text)\n",
    "        print(f\"Cluster Prediction: {cluster}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the sample test function\n",
    "if __name__ == \"__main__\":\n",
    "    run_sample_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6163310,
     "sourceId": 10011249,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
